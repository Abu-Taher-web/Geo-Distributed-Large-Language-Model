{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1d67dc-ef36-485e-9db1-1709a82c16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3856ae79-352f-4a05-893b-0c2254c57288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045290c55e43481489edaf48ecaad46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787970875f8d4b8b8b2c39b888d0964a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc2e5501acf44a8bf31f4ddd49ca198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8a077be95d42b2b0ba11ca0824f617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f081ae36a91d46ce87aa132524e4b900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2b0f3f64564964a20e50ba57a3b7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af41ad1a735455cabcc9f15d14cd1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "\n",
      "I'm a little bit of a nerd. I'm a big nerd. I'm a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also use other variants like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686e4ea6-ef22-466f-b322-9d77cc412965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "{'input_ids': tensor([[15496,    11,   703,   389,   345,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[15496,    11,   703,   389,   345,    30,   314,   892,   356,   460,\n",
      "           466,   428,  1978,   553,   673,   531,    13,   198,   198,  3347,\n",
      "           788,  1234,   866,   607,  6131,   290,  1718,   257,  5852,   379,\n",
      "           262,  3084,    11,  5586,   866,   351,   257,  8212,   319,   607,\n",
      "          1986,    13,   366,  1135,  1183,   766,   703,   345,   821,  1804,\n",
      "           553,   531,   530,   286,   262,  1854,    13, 50256]])\n",
      "Hello, how are you? I think we can do this together,\" she said.\n",
      "\n",
      "She then put down her bag and took a seat at the table, sitting down with a smile on her face. \"We'll see how you're doing,\" said one of the others.\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "input_text = \"Hello, how are you?\"\n",
    "print(input_text)\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(inputs) \n",
    "\n",
    "# Generate output\n",
    "#outputs = model.generate(**inputs)\n",
    "#outputs = model.generate(**inputs, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "outputs = model.generate(**inputs, max_length=200, no_repeat_ngram_size=2, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "\n",
    "print(outputs) \n",
    "\n",
    "# Decode the generated tokens to text\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9716e6c0-004d-4158-b3b6-1fa3e54c53dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load GPT-2 model (quantized for low memory)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Save model in chunks for splitting (if necessary)\n",
    "torch.save(model.state_dict(), \"gpt2_model_split.pth\")\n",
    "# Get the state dictionary of the model\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Print out the keys to see the model layers (for splitting)\n",
    "for key in state_dict.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803115be-977c-4cf7-8973-fae077ccfe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 12 transformer layers, split them into 3 parts (4 layers per part)\n",
    "part1 = {}\n",
    "part2 = {}\n",
    "part3 = {}\n",
    "\n",
    "# Split transformer layers into 3 parts\n",
    "layers = [key for key in state_dict.keys() if \"transformer.h.\" in key]\n",
    "\n",
    "# Divide layers into three parts\n",
    "part1_layers = layers[:4]\n",
    "part2_layers = layers[4:8]\n",
    "part3_layers = layers[8:]\n",
    "\n",
    "# Add layers to each part\n",
    "for layer in part1_layers:\n",
    "    part1[layer] = state_dict[layer]\n",
    "\n",
    "for layer in part2_layers:\n",
    "    part2[layer] = state_dict[layer]\n",
    "\n",
    "for layer in part3_layers:\n",
    "    part3[layer] = state_dict[layer]\n",
    "\n",
    "# Save each part\n",
    "torch.save(part1, \"gpt2_part1.pth\")\n",
    "torch.save(part2, \"gpt2_part2.pth\")\n",
    "torch.save(part3, \"gpt2_part3.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97405209-8366-4f33-8008-1e388151fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully split into two parts.\n"
     ]
    }
   ],
   "source": [
    "# Get all transformer blocks\n",
    "transformer_blocks = model.transformer.h\n",
    "\n",
    "# Split model into two halves\n",
    "midpoint = len(transformer_blocks) // 2\n",
    "part1_blocks = transformer_blocks[:midpoint]\n",
    "part2_blocks = transformer_blocks[midpoint:]\n",
    "\n",
    "# Create two separate model parts\n",
    "model_part1 = torch.nn.Sequential(*part1_blocks)\n",
    "model_part2 = torch.nn.Sequential(*part2_blocks)\n",
    "\n",
    "# Save the two parts separately\n",
    "torch.save(model_part1.state_dict(), \"gpt2_part1_taher.pth\")\n",
    "torch.save(model_part2.state_dict(), \"gpt2_part2_rabiul.pth\")\n",
    "\n",
    "print(\"Model successfully split into two parts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee6d729a-a294-4233-a2f4-34077ef18a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "transformer block: \n",
      " ModuleList(\n",
      "  (0-11): 12 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "midpoint:  6\n",
      "block 1: \n",
      " ModuleList(\n",
      "  (0-5): 6 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "block 2: \n",
      " ModuleList(\n",
      "  (0-5): 6 x GPT2Block(\n",
      "    (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): GPT2Attention(\n",
      "      (c_attn): Conv1D(nf=2304, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=768)\n",
      "      (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (mlp): GPT2MLP(\n",
      "      (c_fc): Conv1D(nf=3072, nx=768)\n",
      "      (c_proj): Conv1D(nf=768, nx=3072)\n",
      "      (act): NewGELUActivation()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "model part 1: \n",
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model part 1: \n",
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-5): 6 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model part 2: \n",
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model part 2: \n",
      " GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-5): 6 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "print(\"model:\\n\", model)\n",
    "\n",
    "# Split the transformer blocks\n",
    "transformer_blocks = model.h  # Access the transformer blocks directly\n",
    "print(\"transformer block: \\n\", transformer_blocks)\n",
    "\n",
    "# Find the midpoint for splitting\n",
    "midpoint = len(transformer_blocks) // 2\n",
    "print(\"midpoint: \", midpoint)\n",
    "\n",
    "# Split the blocks into two parts\n",
    "part1_blocks = transformer_blocks[:midpoint]\n",
    "part2_blocks = transformer_blocks[midpoint:]\n",
    "print(\"block 1: \\n\", part1_blocks)\n",
    "print(\"block 2: \\n\", part2_blocks)\n",
    "\n",
    "# Save each part\n",
    "model_part1 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "print(\"model part 1: \\n\", model_part1)\n",
    "model_part1.h = torch.nn.ModuleList(part1_blocks)  # Replace with the first part\n",
    "torch.save(model_part1.state_dict(), \"gpt2_part1_taher.pth\")\n",
    "print(\"model part 1: \\n\", model_part1)\n",
    "\n",
    "\n",
    "model_part2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "print(\"model part 2: \\n\", model_part2)\n",
    "model_part2.h = torch.nn.ModuleList(part2_blocks)  # Replace with the second part\n",
    "torch.save(model_part2.state_dict(), \"gpt2_part2_rabiul.pth\")\n",
    "print(\"model part 2: \\n\", model_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "554dc8c5-b185-491c-9d4b-2f0ef25fb29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\2623574572.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part1.load_state_dict(torch.load(\"gpt2_part1_taher.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Part1Model:\n\tMissing key(s) in state_dict: \"model.wte.weight\", \"model.wpe.weight\", \"model.h.0.ln_1.weight\", \"model.h.0.ln_1.bias\", \"model.h.0.attn.c_attn.weight\", \"model.h.0.attn.c_attn.bias\", \"model.h.0.attn.c_proj.weight\", \"model.h.0.attn.c_proj.bias\", \"model.h.0.ln_2.weight\", \"model.h.0.ln_2.bias\", \"model.h.0.mlp.c_fc.weight\", \"model.h.0.mlp.c_fc.bias\", \"model.h.0.mlp.c_proj.weight\", \"model.h.0.mlp.c_proj.bias\", \"model.h.1.ln_1.weight\", \"model.h.1.ln_1.bias\", \"model.h.1.attn.c_attn.weight\", \"model.h.1.attn.c_attn.bias\", \"model.h.1.attn.c_proj.weight\", \"model.h.1.attn.c_proj.bias\", \"model.h.1.ln_2.weight\", \"model.h.1.ln_2.bias\", \"model.h.1.mlp.c_fc.weight\", \"model.h.1.mlp.c_fc.bias\", \"model.h.1.mlp.c_proj.weight\", \"model.h.1.mlp.c_proj.bias\", \"model.h.2.ln_1.weight\", \"model.h.2.ln_1.bias\", \"model.h.2.attn.c_attn.weight\", \"model.h.2.attn.c_attn.bias\", \"model.h.2.attn.c_proj.weight\", \"model.h.2.attn.c_proj.bias\", \"model.h.2.ln_2.weight\", \"model.h.2.ln_2.bias\", \"model.h.2.mlp.c_fc.weight\", \"model.h.2.mlp.c_fc.bias\", \"model.h.2.mlp.c_proj.weight\", \"model.h.2.mlp.c_proj.bias\", \"model.h.3.ln_1.weight\", \"model.h.3.ln_1.bias\", \"model.h.3.attn.c_attn.weight\", \"model.h.3.attn.c_attn.bias\", \"model.h.3.attn.c_proj.weight\", \"model.h.3.attn.c_proj.bias\", \"model.h.3.ln_2.weight\", \"model.h.3.ln_2.bias\", \"model.h.3.mlp.c_fc.weight\", \"model.h.3.mlp.c_fc.bias\", \"model.h.3.mlp.c_proj.weight\", \"model.h.3.mlp.c_proj.bias\", \"model.h.4.ln_1.weight\", \"model.h.4.ln_1.bias\", \"model.h.4.attn.c_attn.weight\", \"model.h.4.attn.c_attn.bias\", \"model.h.4.attn.c_proj.weight\", \"model.h.4.attn.c_proj.bias\", \"model.h.4.ln_2.weight\", \"model.h.4.ln_2.bias\", \"model.h.4.mlp.c_fc.weight\", \"model.h.4.mlp.c_fc.bias\", \"model.h.4.mlp.c_proj.weight\", \"model.h.4.mlp.c_proj.bias\", \"model.h.5.ln_1.weight\", \"model.h.5.ln_1.bias\", \"model.h.5.attn.c_attn.weight\", \"model.h.5.attn.c_attn.bias\", \"model.h.5.attn.c_proj.weight\", \"model.h.5.attn.c_proj.bias\", \"model.h.5.ln_2.weight\", \"model.h.5.ln_2.bias\", \"model.h.5.mlp.c_fc.weight\", \"model.h.5.mlp.c_fc.bias\", \"model.h.5.mlp.c_proj.weight\", \"model.h.5.mlp.c_proj.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n\tUnexpected key(s) in state_dict: \"wte.weight\", \"wpe.weight\", \"h.0.ln_1.weight\", \"h.0.ln_1.bias\", \"h.0.attn.c_attn.weight\", \"h.0.attn.c_attn.bias\", \"h.0.attn.c_proj.weight\", \"h.0.attn.c_proj.bias\", \"h.0.ln_2.weight\", \"h.0.ln_2.bias\", \"h.0.mlp.c_fc.weight\", \"h.0.mlp.c_fc.bias\", \"h.0.mlp.c_proj.weight\", \"h.0.mlp.c_proj.bias\", \"h.1.ln_1.weight\", \"h.1.ln_1.bias\", \"h.1.attn.c_attn.weight\", \"h.1.attn.c_attn.bias\", \"h.1.attn.c_proj.weight\", \"h.1.attn.c_proj.bias\", \"h.1.ln_2.weight\", \"h.1.ln_2.bias\", \"h.1.mlp.c_fc.weight\", \"h.1.mlp.c_fc.bias\", \"h.1.mlp.c_proj.weight\", \"h.1.mlp.c_proj.bias\", \"h.2.ln_1.weight\", \"h.2.ln_1.bias\", \"h.2.attn.c_attn.weight\", \"h.2.attn.c_attn.bias\", \"h.2.attn.c_proj.weight\", \"h.2.attn.c_proj.bias\", \"h.2.ln_2.weight\", \"h.2.ln_2.bias\", \"h.2.mlp.c_fc.weight\", \"h.2.mlp.c_fc.bias\", \"h.2.mlp.c_proj.weight\", \"h.2.mlp.c_proj.bias\", \"h.3.ln_1.weight\", \"h.3.ln_1.bias\", \"h.3.attn.c_attn.weight\", \"h.3.attn.c_attn.bias\", \"h.3.attn.c_proj.weight\", \"h.3.attn.c_proj.bias\", \"h.3.ln_2.weight\", \"h.3.ln_2.bias\", \"h.3.mlp.c_fc.weight\", \"h.3.mlp.c_fc.bias\", \"h.3.mlp.c_proj.weight\", \"h.3.mlp.c_proj.bias\", \"h.4.ln_1.weight\", \"h.4.ln_1.bias\", \"h.4.attn.c_attn.weight\", \"h.4.attn.c_attn.bias\", \"h.4.attn.c_proj.weight\", \"h.4.attn.c_proj.bias\", \"h.4.ln_2.weight\", \"h.4.ln_2.bias\", \"h.4.mlp.c_fc.weight\", \"h.4.mlp.c_fc.bias\", \"h.4.mlp.c_proj.weight\", \"h.4.mlp.c_proj.bias\", \"h.5.ln_1.weight\", \"h.5.ln_1.bias\", \"h.5.attn.c_attn.weight\", \"h.5.attn.c_attn.bias\", \"h.5.attn.c_proj.weight\", \"h.5.attn.c_proj.bias\", \"h.5.ln_2.weight\", \"h.5.ln_2.bias\", \"h.5.mlp.c_fc.weight\", \"h.5.mlp.c_fc.bias\", \"h.5.mlp.c_proj.weight\", \"h.5.mlp.c_proj.bias\", \"ln_f.weight\", \"ln_f.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model_part1 \u001b[38;5;241m=\u001b[39m Part1Model()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load the weights for the first part\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel_part1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2_part1_taher.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPart 1 model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Part1Model:\n\tMissing key(s) in state_dict: \"model.wte.weight\", \"model.wpe.weight\", \"model.h.0.ln_1.weight\", \"model.h.0.ln_1.bias\", \"model.h.0.attn.c_attn.weight\", \"model.h.0.attn.c_attn.bias\", \"model.h.0.attn.c_proj.weight\", \"model.h.0.attn.c_proj.bias\", \"model.h.0.ln_2.weight\", \"model.h.0.ln_2.bias\", \"model.h.0.mlp.c_fc.weight\", \"model.h.0.mlp.c_fc.bias\", \"model.h.0.mlp.c_proj.weight\", \"model.h.0.mlp.c_proj.bias\", \"model.h.1.ln_1.weight\", \"model.h.1.ln_1.bias\", \"model.h.1.attn.c_attn.weight\", \"model.h.1.attn.c_attn.bias\", \"model.h.1.attn.c_proj.weight\", \"model.h.1.attn.c_proj.bias\", \"model.h.1.ln_2.weight\", \"model.h.1.ln_2.bias\", \"model.h.1.mlp.c_fc.weight\", \"model.h.1.mlp.c_fc.bias\", \"model.h.1.mlp.c_proj.weight\", \"model.h.1.mlp.c_proj.bias\", \"model.h.2.ln_1.weight\", \"model.h.2.ln_1.bias\", \"model.h.2.attn.c_attn.weight\", \"model.h.2.attn.c_attn.bias\", \"model.h.2.attn.c_proj.weight\", \"model.h.2.attn.c_proj.bias\", \"model.h.2.ln_2.weight\", \"model.h.2.ln_2.bias\", \"model.h.2.mlp.c_fc.weight\", \"model.h.2.mlp.c_fc.bias\", \"model.h.2.mlp.c_proj.weight\", \"model.h.2.mlp.c_proj.bias\", \"model.h.3.ln_1.weight\", \"model.h.3.ln_1.bias\", \"model.h.3.attn.c_attn.weight\", \"model.h.3.attn.c_attn.bias\", \"model.h.3.attn.c_proj.weight\", \"model.h.3.attn.c_proj.bias\", \"model.h.3.ln_2.weight\", \"model.h.3.ln_2.bias\", \"model.h.3.mlp.c_fc.weight\", \"model.h.3.mlp.c_fc.bias\", \"model.h.3.mlp.c_proj.weight\", \"model.h.3.mlp.c_proj.bias\", \"model.h.4.ln_1.weight\", \"model.h.4.ln_1.bias\", \"model.h.4.attn.c_attn.weight\", \"model.h.4.attn.c_attn.bias\", \"model.h.4.attn.c_proj.weight\", \"model.h.4.attn.c_proj.bias\", \"model.h.4.ln_2.weight\", \"model.h.4.ln_2.bias\", \"model.h.4.mlp.c_fc.weight\", \"model.h.4.mlp.c_fc.bias\", \"model.h.4.mlp.c_proj.weight\", \"model.h.4.mlp.c_proj.bias\", \"model.h.5.ln_1.weight\", \"model.h.5.ln_1.bias\", \"model.h.5.attn.c_attn.weight\", \"model.h.5.attn.c_attn.bias\", \"model.h.5.attn.c_proj.weight\", \"model.h.5.attn.c_proj.bias\", \"model.h.5.ln_2.weight\", \"model.h.5.ln_2.bias\", \"model.h.5.mlp.c_fc.weight\", \"model.h.5.mlp.c_fc.bias\", \"model.h.5.mlp.c_proj.weight\", \"model.h.5.mlp.c_proj.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n\tUnexpected key(s) in state_dict: \"wte.weight\", \"wpe.weight\", \"h.0.ln_1.weight\", \"h.0.ln_1.bias\", \"h.0.attn.c_attn.weight\", \"h.0.attn.c_attn.bias\", \"h.0.attn.c_proj.weight\", \"h.0.attn.c_proj.bias\", \"h.0.ln_2.weight\", \"h.0.ln_2.bias\", \"h.0.mlp.c_fc.weight\", \"h.0.mlp.c_fc.bias\", \"h.0.mlp.c_proj.weight\", \"h.0.mlp.c_proj.bias\", \"h.1.ln_1.weight\", \"h.1.ln_1.bias\", \"h.1.attn.c_attn.weight\", \"h.1.attn.c_attn.bias\", \"h.1.attn.c_proj.weight\", \"h.1.attn.c_proj.bias\", \"h.1.ln_2.weight\", \"h.1.ln_2.bias\", \"h.1.mlp.c_fc.weight\", \"h.1.mlp.c_fc.bias\", \"h.1.mlp.c_proj.weight\", \"h.1.mlp.c_proj.bias\", \"h.2.ln_1.weight\", \"h.2.ln_1.bias\", \"h.2.attn.c_attn.weight\", \"h.2.attn.c_attn.bias\", \"h.2.attn.c_proj.weight\", \"h.2.attn.c_proj.bias\", \"h.2.ln_2.weight\", \"h.2.ln_2.bias\", \"h.2.mlp.c_fc.weight\", \"h.2.mlp.c_fc.bias\", \"h.2.mlp.c_proj.weight\", \"h.2.mlp.c_proj.bias\", \"h.3.ln_1.weight\", \"h.3.ln_1.bias\", \"h.3.attn.c_attn.weight\", \"h.3.attn.c_attn.bias\", \"h.3.attn.c_proj.weight\", \"h.3.attn.c_proj.bias\", \"h.3.ln_2.weight\", \"h.3.ln_2.bias\", \"h.3.mlp.c_fc.weight\", \"h.3.mlp.c_fc.bias\", \"h.3.mlp.c_proj.weight\", \"h.3.mlp.c_proj.bias\", \"h.4.ln_1.weight\", \"h.4.ln_1.bias\", \"h.4.attn.c_attn.weight\", \"h.4.attn.c_attn.bias\", \"h.4.attn.c_proj.weight\", \"h.4.attn.c_proj.bias\", \"h.4.ln_2.weight\", \"h.4.ln_2.bias\", \"h.4.mlp.c_fc.weight\", \"h.4.mlp.c_fc.bias\", \"h.4.mlp.c_proj.weight\", \"h.4.mlp.c_proj.bias\", \"h.5.ln_1.weight\", \"h.5.ln_1.bias\", \"h.5.attn.c_attn.weight\", \"h.5.attn.c_attn.bias\", \"h.5.attn.c_proj.weight\", \"h.5.attn.c_proj.bias\", \"h.5.ln_2.weight\", \"h.5.ln_2.bias\", \"h.5.mlp.c_fc.weight\", \"h.5.mlp.c_fc.bias\", \"h.5.mlp.c_proj.weight\", \"h.5.mlp.c_proj.bias\", \"ln_f.weight\", \"ln_f.bias\". "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the architecture for the first part of the model\n",
    "class Part1Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the full GPT-2 model architecture\n",
    "        self.model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        # Retain only the first half of the transformer blocks\n",
    "        self.model.h = torch.nn.ModuleList(\n",
    "            list(self.model.h)[:len(self.model.h) // 2]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "# Instantiate the first part of the model\n",
    "model_part1 = Part1Model()\n",
    "\n",
    "# Load the weights for the first part\n",
    "model_part1.load_state_dict(torch.load(\"gpt2_part1_taher.pth\"))\n",
    "\n",
    "print(\"Part 1 model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5014de0-0cc2-4520-8c11-6297f8c69faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\1124008301.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part2.load_state_dict(torch.load(\"gpt2_part2_rabiul.pth\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Part2Model:\n\tMissing key(s) in state_dict: \"model.wte.weight\", \"model.wpe.weight\", \"model.h.0.ln_1.weight\", \"model.h.0.ln_1.bias\", \"model.h.0.attn.c_attn.weight\", \"model.h.0.attn.c_attn.bias\", \"model.h.0.attn.c_proj.weight\", \"model.h.0.attn.c_proj.bias\", \"model.h.0.ln_2.weight\", \"model.h.0.ln_2.bias\", \"model.h.0.mlp.c_fc.weight\", \"model.h.0.mlp.c_fc.bias\", \"model.h.0.mlp.c_proj.weight\", \"model.h.0.mlp.c_proj.bias\", \"model.h.1.ln_1.weight\", \"model.h.1.ln_1.bias\", \"model.h.1.attn.c_attn.weight\", \"model.h.1.attn.c_attn.bias\", \"model.h.1.attn.c_proj.weight\", \"model.h.1.attn.c_proj.bias\", \"model.h.1.ln_2.weight\", \"model.h.1.ln_2.bias\", \"model.h.1.mlp.c_fc.weight\", \"model.h.1.mlp.c_fc.bias\", \"model.h.1.mlp.c_proj.weight\", \"model.h.1.mlp.c_proj.bias\", \"model.h.2.ln_1.weight\", \"model.h.2.ln_1.bias\", \"model.h.2.attn.c_attn.weight\", \"model.h.2.attn.c_attn.bias\", \"model.h.2.attn.c_proj.weight\", \"model.h.2.attn.c_proj.bias\", \"model.h.2.ln_2.weight\", \"model.h.2.ln_2.bias\", \"model.h.2.mlp.c_fc.weight\", \"model.h.2.mlp.c_fc.bias\", \"model.h.2.mlp.c_proj.weight\", \"model.h.2.mlp.c_proj.bias\", \"model.h.3.ln_1.weight\", \"model.h.3.ln_1.bias\", \"model.h.3.attn.c_attn.weight\", \"model.h.3.attn.c_attn.bias\", \"model.h.3.attn.c_proj.weight\", \"model.h.3.attn.c_proj.bias\", \"model.h.3.ln_2.weight\", \"model.h.3.ln_2.bias\", \"model.h.3.mlp.c_fc.weight\", \"model.h.3.mlp.c_fc.bias\", \"model.h.3.mlp.c_proj.weight\", \"model.h.3.mlp.c_proj.bias\", \"model.h.4.ln_1.weight\", \"model.h.4.ln_1.bias\", \"model.h.4.attn.c_attn.weight\", \"model.h.4.attn.c_attn.bias\", \"model.h.4.attn.c_proj.weight\", \"model.h.4.attn.c_proj.bias\", \"model.h.4.ln_2.weight\", \"model.h.4.ln_2.bias\", \"model.h.4.mlp.c_fc.weight\", \"model.h.4.mlp.c_fc.bias\", \"model.h.4.mlp.c_proj.weight\", \"model.h.4.mlp.c_proj.bias\", \"model.h.5.ln_1.weight\", \"model.h.5.ln_1.bias\", \"model.h.5.attn.c_attn.weight\", \"model.h.5.attn.c_attn.bias\", \"model.h.5.attn.c_proj.weight\", \"model.h.5.attn.c_proj.bias\", \"model.h.5.ln_2.weight\", \"model.h.5.ln_2.bias\", \"model.h.5.mlp.c_fc.weight\", \"model.h.5.mlp.c_fc.bias\", \"model.h.5.mlp.c_proj.weight\", \"model.h.5.mlp.c_proj.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n\tUnexpected key(s) in state_dict: \"wte.weight\", \"wpe.weight\", \"h.0.ln_1.weight\", \"h.0.ln_1.bias\", \"h.0.attn.c_attn.weight\", \"h.0.attn.c_attn.bias\", \"h.0.attn.c_proj.weight\", \"h.0.attn.c_proj.bias\", \"h.0.ln_2.weight\", \"h.0.ln_2.bias\", \"h.0.mlp.c_fc.weight\", \"h.0.mlp.c_fc.bias\", \"h.0.mlp.c_proj.weight\", \"h.0.mlp.c_proj.bias\", \"h.1.ln_1.weight\", \"h.1.ln_1.bias\", \"h.1.attn.c_attn.weight\", \"h.1.attn.c_attn.bias\", \"h.1.attn.c_proj.weight\", \"h.1.attn.c_proj.bias\", \"h.1.ln_2.weight\", \"h.1.ln_2.bias\", \"h.1.mlp.c_fc.weight\", \"h.1.mlp.c_fc.bias\", \"h.1.mlp.c_proj.weight\", \"h.1.mlp.c_proj.bias\", \"h.2.ln_1.weight\", \"h.2.ln_1.bias\", \"h.2.attn.c_attn.weight\", \"h.2.attn.c_attn.bias\", \"h.2.attn.c_proj.weight\", \"h.2.attn.c_proj.bias\", \"h.2.ln_2.weight\", \"h.2.ln_2.bias\", \"h.2.mlp.c_fc.weight\", \"h.2.mlp.c_fc.bias\", \"h.2.mlp.c_proj.weight\", \"h.2.mlp.c_proj.bias\", \"h.3.ln_1.weight\", \"h.3.ln_1.bias\", \"h.3.attn.c_attn.weight\", \"h.3.attn.c_attn.bias\", \"h.3.attn.c_proj.weight\", \"h.3.attn.c_proj.bias\", \"h.3.ln_2.weight\", \"h.3.ln_2.bias\", \"h.3.mlp.c_fc.weight\", \"h.3.mlp.c_fc.bias\", \"h.3.mlp.c_proj.weight\", \"h.3.mlp.c_proj.bias\", \"h.4.ln_1.weight\", \"h.4.ln_1.bias\", \"h.4.attn.c_attn.weight\", \"h.4.attn.c_attn.bias\", \"h.4.attn.c_proj.weight\", \"h.4.attn.c_proj.bias\", \"h.4.ln_2.weight\", \"h.4.ln_2.bias\", \"h.4.mlp.c_fc.weight\", \"h.4.mlp.c_fc.bias\", \"h.4.mlp.c_proj.weight\", \"h.4.mlp.c_proj.bias\", \"h.5.ln_1.weight\", \"h.5.ln_1.bias\", \"h.5.attn.c_attn.weight\", \"h.5.attn.c_attn.bias\", \"h.5.attn.c_proj.weight\", \"h.5.attn.c_proj.bias\", \"h.5.ln_2.weight\", \"h.5.ln_2.bias\", \"h.5.mlp.c_fc.weight\", \"h.5.mlp.c_fc.bias\", \"h.5.mlp.c_proj.weight\", \"h.5.mlp.c_proj.bias\", \"ln_f.weight\", \"ln_f.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m model_part2 \u001b[38;5;241m=\u001b[39m Part2Model()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load the weights for the second part\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodel_part2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2_part2_rabiul.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPart 2 model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Part2Model:\n\tMissing key(s) in state_dict: \"model.wte.weight\", \"model.wpe.weight\", \"model.h.0.ln_1.weight\", \"model.h.0.ln_1.bias\", \"model.h.0.attn.c_attn.weight\", \"model.h.0.attn.c_attn.bias\", \"model.h.0.attn.c_proj.weight\", \"model.h.0.attn.c_proj.bias\", \"model.h.0.ln_2.weight\", \"model.h.0.ln_2.bias\", \"model.h.0.mlp.c_fc.weight\", \"model.h.0.mlp.c_fc.bias\", \"model.h.0.mlp.c_proj.weight\", \"model.h.0.mlp.c_proj.bias\", \"model.h.1.ln_1.weight\", \"model.h.1.ln_1.bias\", \"model.h.1.attn.c_attn.weight\", \"model.h.1.attn.c_attn.bias\", \"model.h.1.attn.c_proj.weight\", \"model.h.1.attn.c_proj.bias\", \"model.h.1.ln_2.weight\", \"model.h.1.ln_2.bias\", \"model.h.1.mlp.c_fc.weight\", \"model.h.1.mlp.c_fc.bias\", \"model.h.1.mlp.c_proj.weight\", \"model.h.1.mlp.c_proj.bias\", \"model.h.2.ln_1.weight\", \"model.h.2.ln_1.bias\", \"model.h.2.attn.c_attn.weight\", \"model.h.2.attn.c_attn.bias\", \"model.h.2.attn.c_proj.weight\", \"model.h.2.attn.c_proj.bias\", \"model.h.2.ln_2.weight\", \"model.h.2.ln_2.bias\", \"model.h.2.mlp.c_fc.weight\", \"model.h.2.mlp.c_fc.bias\", \"model.h.2.mlp.c_proj.weight\", \"model.h.2.mlp.c_proj.bias\", \"model.h.3.ln_1.weight\", \"model.h.3.ln_1.bias\", \"model.h.3.attn.c_attn.weight\", \"model.h.3.attn.c_attn.bias\", \"model.h.3.attn.c_proj.weight\", \"model.h.3.attn.c_proj.bias\", \"model.h.3.ln_2.weight\", \"model.h.3.ln_2.bias\", \"model.h.3.mlp.c_fc.weight\", \"model.h.3.mlp.c_fc.bias\", \"model.h.3.mlp.c_proj.weight\", \"model.h.3.mlp.c_proj.bias\", \"model.h.4.ln_1.weight\", \"model.h.4.ln_1.bias\", \"model.h.4.attn.c_attn.weight\", \"model.h.4.attn.c_attn.bias\", \"model.h.4.attn.c_proj.weight\", \"model.h.4.attn.c_proj.bias\", \"model.h.4.ln_2.weight\", \"model.h.4.ln_2.bias\", \"model.h.4.mlp.c_fc.weight\", \"model.h.4.mlp.c_fc.bias\", \"model.h.4.mlp.c_proj.weight\", \"model.h.4.mlp.c_proj.bias\", \"model.h.5.ln_1.weight\", \"model.h.5.ln_1.bias\", \"model.h.5.attn.c_attn.weight\", \"model.h.5.attn.c_attn.bias\", \"model.h.5.attn.c_proj.weight\", \"model.h.5.attn.c_proj.bias\", \"model.h.5.ln_2.weight\", \"model.h.5.ln_2.bias\", \"model.h.5.mlp.c_fc.weight\", \"model.h.5.mlp.c_fc.bias\", \"model.h.5.mlp.c_proj.weight\", \"model.h.5.mlp.c_proj.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n\tUnexpected key(s) in state_dict: \"wte.weight\", \"wpe.weight\", \"h.0.ln_1.weight\", \"h.0.ln_1.bias\", \"h.0.attn.c_attn.weight\", \"h.0.attn.c_attn.bias\", \"h.0.attn.c_proj.weight\", \"h.0.attn.c_proj.bias\", \"h.0.ln_2.weight\", \"h.0.ln_2.bias\", \"h.0.mlp.c_fc.weight\", \"h.0.mlp.c_fc.bias\", \"h.0.mlp.c_proj.weight\", \"h.0.mlp.c_proj.bias\", \"h.1.ln_1.weight\", \"h.1.ln_1.bias\", \"h.1.attn.c_attn.weight\", \"h.1.attn.c_attn.bias\", \"h.1.attn.c_proj.weight\", \"h.1.attn.c_proj.bias\", \"h.1.ln_2.weight\", \"h.1.ln_2.bias\", \"h.1.mlp.c_fc.weight\", \"h.1.mlp.c_fc.bias\", \"h.1.mlp.c_proj.weight\", \"h.1.mlp.c_proj.bias\", \"h.2.ln_1.weight\", \"h.2.ln_1.bias\", \"h.2.attn.c_attn.weight\", \"h.2.attn.c_attn.bias\", \"h.2.attn.c_proj.weight\", \"h.2.attn.c_proj.bias\", \"h.2.ln_2.weight\", \"h.2.ln_2.bias\", \"h.2.mlp.c_fc.weight\", \"h.2.mlp.c_fc.bias\", \"h.2.mlp.c_proj.weight\", \"h.2.mlp.c_proj.bias\", \"h.3.ln_1.weight\", \"h.3.ln_1.bias\", \"h.3.attn.c_attn.weight\", \"h.3.attn.c_attn.bias\", \"h.3.attn.c_proj.weight\", \"h.3.attn.c_proj.bias\", \"h.3.ln_2.weight\", \"h.3.ln_2.bias\", \"h.3.mlp.c_fc.weight\", \"h.3.mlp.c_fc.bias\", \"h.3.mlp.c_proj.weight\", \"h.3.mlp.c_proj.bias\", \"h.4.ln_1.weight\", \"h.4.ln_1.bias\", \"h.4.attn.c_attn.weight\", \"h.4.attn.c_attn.bias\", \"h.4.attn.c_proj.weight\", \"h.4.attn.c_proj.bias\", \"h.4.ln_2.weight\", \"h.4.ln_2.bias\", \"h.4.mlp.c_fc.weight\", \"h.4.mlp.c_fc.bias\", \"h.4.mlp.c_proj.weight\", \"h.4.mlp.c_proj.bias\", \"h.5.ln_1.weight\", \"h.5.ln_1.bias\", \"h.5.attn.c_attn.weight\", \"h.5.attn.c_attn.bias\", \"h.5.attn.c_proj.weight\", \"h.5.attn.c_proj.bias\", \"h.5.ln_2.weight\", \"h.5.ln_2.bias\", \"h.5.mlp.c_fc.weight\", \"h.5.mlp.c_fc.bias\", \"h.5.mlp.c_proj.weight\", \"h.5.mlp.c_proj.bias\", \"ln_f.weight\", \"ln_f.bias\". "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the architecture for the second part of the model\n",
    "class Part2Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the full GPT-2 model architecture\n",
    "        self.model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        # Retain only the second half of the transformer blocks\n",
    "        self.model.h = torch.nn.ModuleList(\n",
    "            list(self.model.h)[len(self.model.h) // 2:]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "# Instantiate the second part of the model\n",
    "model_part2 = Part2Model()\n",
    "\n",
    "# Load the weights for the second part\n",
    "model_part2.load_state_dict(torch.load(\"gpt2_part2_rabiul.pth\"))\n",
    "\n",
    "print(\"Part 2 model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27baf79a-3367-487b-9491-6311f2cd77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the full GPT-2 model\n",
    "full_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Split the transformer blocks\n",
    "transformer_blocks = full_model.h\n",
    "midpoint = len(transformer_blocks) // 2\n",
    "\n",
    "# First part: Embeddings + First half of transformer blocks\n",
    "class GPT2Part1(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.wte = original_model.wte  # Word embeddings\n",
    "        self.wpe = original_model.wpe  # Positional embeddings\n",
    "        self.drop = original_model.drop  # Dropout\n",
    "        self.h = torch.nn.ModuleList(original_model.h[:midpoint])  # First half of transformer blocks\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Compute embeddings\n",
    "        input_embeds = self.wte(input_ids) + self.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        hidden_states = self.drop(input_embeds)\n",
    "\n",
    "        # Pass through the first half of the transformer blocks\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# Second part: Second half of transformer blocks + Final layer norm\n",
    "class GPT2Part2(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.h = torch.nn.ModuleList(original_model.h[midpoint:])  # Second half of transformer blocks\n",
    "        self.ln_f = original_model.ln_f  # Final layer norm\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Pass through the second half of the transformer blocks\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Apply final layer norm\n",
    "        output_states = self.ln_f(hidden_states)\n",
    "        return output_states\n",
    "\n",
    "\n",
    "# Create part 1 and part 2\n",
    "model_part1 = GPT2Part1(full_model, midpoint)\n",
    "model_part2 = GPT2Part2(full_model, midpoint)\n",
    "\n",
    "# Save each part\n",
    "torch.save(model_part1.state_dict(), \"gpt2_part1_taher2.pth\")\n",
    "torch.save(model_part2.state_dict(), \"gpt2_part2_rabiul2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99d5d398-f748-4fdb-b391-325b1ba8c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\1669493709.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part1.load_state_dict(torch.load(\"gpt2_part1_taher2.pth\"))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\1669493709.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part2.load_state_dict(torch.load(\"gpt2_part2_rabiul2.pth\"))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "layer_norm(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Step 1: Pass input through part 1\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_part1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Step 2: Pass output of part 1 to part 2\u001b[39;00m\n\u001b[0;32m     62\u001b[0m output_states \u001b[38;5;241m=\u001b[39m model_part2(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m, in \u001b[0;36mGPT2Part1.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(input_embeds)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh:\n\u001b[1;32m---> 17\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:403\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[0;32m    402\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 403\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[0;32m    405\u001b[0m         hidden_states,\n\u001b[0;32m    406\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    411\u001b[0m     )\n\u001b[0;32m    412\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: layer_norm(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Define the GPT2Part1 and GPT2Part2 classes (reuse the classes from the splitting code)\n",
    "class GPT2Part1(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.wte = original_model.wte\n",
    "        self.wpe = original_model.wpe\n",
    "        self.drop = original_model.drop\n",
    "        self.h = torch.nn.ModuleList(original_model.h[:midpoint])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        input_embeds = self.wte(input_ids) + self.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        hidden_states = self.drop(input_embeds)\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Part2(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.h = torch.nn.ModuleList(original_model.h[midpoint:])\n",
    "        self.ln_f = original_model.ln_f\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "        output_states = self.ln_f(hidden_states)\n",
    "        return output_states\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the saved models\n",
    "full_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "midpoint = len(full_model.h) // 2\n",
    "\n",
    "model_part1 = GPT2Part1(full_model, midpoint)\n",
    "model_part1.load_state_dict(torch.load(\"gpt2_part1_taher2.pth\"))\n",
    "\n",
    "model_part2 = GPT2Part2(full_model, midpoint)\n",
    "model_part2.load_state_dict(torch.load(\"gpt2_part2_rabiul2.pth\"))\n",
    "\n",
    "# Put the models in evaluation mode\n",
    "model_part1.eval()\n",
    "model_part2.eval()\n",
    "\n",
    "# Example input text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Step 1: Pass input through part 1\n",
    "hidden_states = model_part1(input_ids)\n",
    "\n",
    "# Step 2: Pass output of part 1 to part 2\n",
    "output_states = model_part2(hidden_states)\n",
    "\n",
    "# Print the shape of the final output\n",
    "print(\"Final output shape:\", output_states.shape)\n",
    "\n",
    "# Optional: Decode the output (depends on your usage, may require fine-tuning for text generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3f84cc7-6ce0-4bff-96ba-a768b6455717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Define the first part of GPT-2\n",
    "class GPT2Part1(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.wte = original_model.wte  # Word embeddings\n",
    "        self.wpe = original_model.wpe  # Positional embeddings\n",
    "        self.drop = original_model.drop  # Dropout\n",
    "        self.h = torch.nn.ModuleList(original_model.h[:midpoint])  # First half of transformer blocks\n",
    "        self.ln_f = original_model.ln_f  # Final normalization layer (for splitting)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Compute embeddings\n",
    "        input_embeds = self.wte(input_ids) + self.wpe(torch.arange(input_ids.size(1), device=input_ids.device))\n",
    "        hidden_states = self.drop(input_embeds)\n",
    "\n",
    "        # Pass through the first half of the transformer blocks\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Apply normalization before passing to the second part\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# Define the second part of GPT-2\n",
    "class GPT2Part2(torch.nn.Module):\n",
    "    def __init__(self, original_model, midpoint):\n",
    "        super().__init__()\n",
    "        self.h = torch.nn.ModuleList(original_model.h[midpoint:])  # Second half of transformer blocks\n",
    "        self.ln_f = original_model.ln_f  # Final normalization layer (retained in case of final output)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Pass through the second half of the transformer blocks\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # Final normalization\n",
    "        output_states = self.ln_f(hidden_states)\n",
    "        return output_states\n",
    "\n",
    "\n",
    "# Load the original GPT-2 model\n",
    "full_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Split the model\n",
    "midpoint = len(full_model.h) // 2\n",
    "model_part1 = GPT2Part1(full_model, midpoint)\n",
    "model_part2 = GPT2Part2(full_model, midpoint)\n",
    "\n",
    "# Save the models\n",
    "torch.save(model_part1.state_dict(), \"gpt2_part111.pth\")\n",
    "torch.save(model_part2.state_dict(), \"gpt2_part222.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a332051f-646a-4df5-81d5-dc72c984a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\3452869986.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part1.load_state_dict(torch.load(\"gpt2_part111.pth\"))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_4324\\3452869986.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part2.load_state_dict(torch.load(\"gpt2_part222.pth\"))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "layer_norm(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Inference: Part 1\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_part1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Inference: Part 2\u001b[39;00m\n\u001b[0;32m     21\u001b[0m output_states \u001b[38;5;241m=\u001b[39m model_part2(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m, in \u001b[0;36mGPT2Part1.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Pass through the first half of the transformer blocks\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh:\n\u001b[1;32m---> 21\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply normalization before passing to the second part\u001b[39;00m\n\u001b[0;32m     24\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:403\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor], Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]]:\n\u001b[0;32m    402\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 403\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[0;32m    405\u001b[0m         hidden_states,\n\u001b[0;32m    406\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    411\u001b[0m     )\n\u001b[0;32m    412\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: layer_norm(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load split models\n",
    "model_part1.load_state_dict(torch.load(\"gpt2_part111.pth\"))\n",
    "model_part2.load_state_dict(torch.load(\"gpt2_part222.pth\"))\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model_part1.eval()\n",
    "model_part2.eval()\n",
    "\n",
    "# Example input text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "print(input_ids) \n",
    "\n",
    "# Inference: Part 1\n",
    "hidden_states = model_part1(input_ids)\n",
    "\n",
    "# Inference: Part 2\n",
    "output_states = model_part2(hidden_states)\n",
    "\n",
    "# Print the output shape\n",
    "print(\"Final output shape:\", output_states.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
