{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec0daa9-a4ab-49e3-b3cd-d8ab9a58bf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d110ec7c41e948b09ad6d6f19cf57bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-125M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2118bb0591c6459cb71c393cf09039e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3124bf593e4774af8b1b2abdac7014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2755d9b9df2b4d3ea9e4d77df3f58dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84db9a938a9e4bb09ac84cbd87f8bde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5542345fc1724541a25824dfa8cbe2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc6523b514f4c0686743e6f37a65614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_12492\\3504667923.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part1.load_state_dict(torch.load(\"gpt_neo_part1.pth\"))\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_12492\\3504667923.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_part2.load_state_dict(torch.load(\"gpt_neo_part2.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-Neo-125M model\n",
    "model = GPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Tokenizer for GPT-Neo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Identify a suitable split point (e.g., after 6 transformer blocks out of 12)\n",
    "split_point = 6\n",
    "\n",
    "# Define Part 1 of the model (embedding layers + first few blocks)\n",
    "class GPTNeoPart1(torch.nn.Module):\n",
    "    def __init__(self, original_model, split_point):\n",
    "        super().__init__()\n",
    "        self.wte = original_model.wte  # Word embeddings\n",
    "        self.h = torch.nn.ModuleList(original_model.h[:split_point])  # First half of the transformer blocks\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Compute embeddings\n",
    "        input_embeds = self.wte(input_ids)\n",
    "        hidden_states = input_embeds\n",
    "\n",
    "        # Pass through the transformer blocks in part 1\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)[0]  # Extract hidden states\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# Define Part 2 of the model (remaining blocks + final layer norm)\n",
    "class GPTNeoPart2(torch.nn.Module):\n",
    "    def __init__(self, original_model, split_point):\n",
    "        super().__init__()\n",
    "        self.h = torch.nn.ModuleList(original_model.h[split_point:])  # Remaining transformer blocks\n",
    "        self.ln_f = original_model.ln_f  # Final layer norm\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Pass through the remaining transformer blocks\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states, attention_mask=attention_mask)[0]  # Extract hidden states\n",
    "\n",
    "        # Apply final normalization\n",
    "        output_states = self.ln_f(hidden_states)\n",
    "        return output_states\n",
    "\n",
    "\n",
    "# Create the split models\n",
    "model_part1 = GPTNeoPart1(model, split_point)\n",
    "model_part2 = GPTNeoPart2(model, split_point)\n",
    "\n",
    "# Save the split models\n",
    "torch.save(model_part1.state_dict(), \"gpt_neo_part1.pth\")\n",
    "torch.save(model_part2.state_dict(), \"gpt_neo_part2.pth\")\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# Reload the models for inference\n",
    "model_part1 = GPTNeoPart1(model, split_point)\n",
    "model_part1.load_state_dict(torch.load(\"gpt_neo_part1.pth\"))\n",
    "model_part1.eval()\n",
    "\n",
    "model_part2 = GPTNeoPart2(model, split_point)\n",
    "model_part2.load_state_dict(torch.load(\"gpt_neo_part2.pth\"))\n",
    "model_part2.eval()\n",
    "print(\"Models loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec0c261-973f-4097-b437-bd5abb86a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "text = \"How are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Part 1 inference\n",
    "hidden_states = model_part1(input_ids)\n",
    "\n",
    "# Part 2 inference\n",
    "output_states = model_part2(hidden_states)\n",
    "\n",
    "# Print final output shape\n",
    "print(\"Final output shape:\", output_states.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23168641-d96a-4c13-9f6a-5a7bd849429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: How are you?\n",
      "\n",
      "You have been diagnosed with a serious neurological condition, known as \"waking eye syndrome\". This is when your vision starts to fail and you have trouble seeing or seeing things. You may have a blurred vision, or even\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "causal_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Example input text\n",
    "text = \"How are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Generate text using the full model\n",
    "generated_ids = causal_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=50,            # Set max tokens in the response\n",
    "    temperature=0.7,          # Sampling temperature for randomness\n",
    "    top_k=50,                 # Top-k sampling\n",
    "    top_p=0.9,                # Nucleus sampling (cumulative probability)\n",
    "    repetition_penalty=1.2,   # Penalize repeated tokens\n",
    "    do_sample=True,           # Enable sampling\n",
    ")\n",
    "\n",
    "# Decode generated token IDs into text\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba547e87-0ea3-44d9-bd3a-ef6b71ec9818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "Explain why ChatGPT sometimes makes mistakes. Consider the limitations of large language models and training data.\n",
      "\n",
      "GENERATED TEXT:\n",
      "Explain why ChatGPT sometimes makes mistakes. Consider the limitations of large language models and training data.\n",
      " as\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " * �\n",
      " \" (\n",
      " \" ( in\n",
      "\n",
      " � (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " �\n",
      " (� (\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " (\n",
      "\n",
      "\n",
      " �\n",
      "\n",
      "\n",
      " * * * � in\n",
      "\n",
      "\n",
      "\n",
      " * �\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ( ( ( ( �\n",
      " * �\n",
      " (\n",
      " �: �\n",
      "� * �\n",
      " �\n",
      "\n",
      "\n",
      " � in�\n",
      " � �\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoModel, GPT2Tokenizer\n",
    "\n",
    "# ----- 1) Load base model & tokenizer -----\n",
    "base_model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "base_model = GPTNeoModel.from_pretrained(base_model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Some GPT-Neo tokenizers lack a pad_token, so reuse eos_token:\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ----- 2) Define split modules -----\n",
    "class GPTNeoPart1(torch.nn.Module):\n",
    "    def __init__(self, original_model, split_point):\n",
    "        super().__init__()\n",
    "        self.wte = original_model.wte  # word embedding\n",
    "        self.h = torch.nn.ModuleList(original_model.h[:split_point])  # first blocks\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids shape: [batch_size, seq_len]\n",
    "        inputs_embeds = self.wte(input_ids)  # [batch_size, seq_len, hidden_dim]\n",
    "        hidden_states = inputs_embeds\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "        return hidden_states\n",
    "\n",
    "class GPTNeoPart2(torch.nn.Module):\n",
    "    def __init__(self, original_model, split_point):\n",
    "        super().__init__()\n",
    "        self.h = torch.nn.ModuleList(original_model.h[split_point:])  # remaining blocks\n",
    "        self.ln_f = original_model.ln_f  # final layer norm\n",
    "\n",
    "        hidden_size = original_model.config.hidden_size\n",
    "        vocab_size = original_model.config.vocab_size\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = torch.nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        # tie lm_head weights to the input embedding (wte)\n",
    "        self.lm_head.weight = original_model.wte.weight\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        for block in self.h:\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        logits = self.lm_head(hidden_states)  # [batch_size, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# ----- 3) Instantiate the split model parts -----\n",
    "split_point = 6  # for GPT-Neo-125M, which has 12 total blocks\n",
    "model_part1 = GPTNeoPart1(base_model, split_point).eval()\n",
    "model_part2 = GPTNeoPart2(base_model, split_point).eval()\n",
    "\n",
    "# ----- 4) Autoregressive generation with top-k / top-p sampling -----\n",
    "@torch.no_grad()\n",
    "def generate_autoregressive(\n",
    "    model1,\n",
    "    model2,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Autoregressively generates text using two-part GPT-Neo.\n",
    "    \"\"\"\n",
    "    # Tokenize the initial prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids  # shape: [1, seq_len]\n",
    "    generated_ids = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 1) Pass full sequence through Part1\n",
    "        hidden_states = model1(generated_ids)\n",
    "        # 2) Pass the resulting hidden states to Part2 for logits\n",
    "        logits = model2(hidden_states)  # [1, seq_len, vocab_size]\n",
    "\n",
    "        # Focus on the last token's logits\n",
    "        next_token_logits = logits[:, -1, :]  # [1, vocab_size]\n",
    "\n",
    "        # -- Apply temperature --\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # -- Top-k filtering --\n",
    "        if top_k is not None and top_k > 0:\n",
    "            top_k = min(top_k, next_token_logits.size(-1))  # Safety\n",
    "            # Get top_k logits\n",
    "            values_to_keep, _ = torch.topk(next_token_logits, top_k)\n",
    "            min_val = values_to_keep[0, -1]  # smallest logit in top_k\n",
    "            next_token_logits[next_token_logits < min_val] = -float('Inf')\n",
    "\n",
    "        # -- Top-p (nucleus) filtering --\n",
    "        if top_p is not None and top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "            # find cutoff index\n",
    "            cutoff_idx = torch.sum(cumulative_probs <= top_p).item()\n",
    "            # set everything after cutoff to -Inf\n",
    "            if cutoff_idx < sorted_logits.size(-1):\n",
    "                sorted_logits[0, cutoff_idx+1:] = -float('Inf')\n",
    "            # map back\n",
    "            next_token_logits.fill_(-float('Inf'))\n",
    "            next_token_logits.scatter_(1, sorted_indices, sorted_logits)\n",
    "\n",
    "        # -- Sample from the filtered distribution --\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)  # shape: [1, 1]\n",
    "        \n",
    "        # Append next token\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        # OPTIONAL: stop at EOS if desired\n",
    "        if tokenizer.eos_token_id is not None and next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode everything (prompt + newly generated tokens)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ----- 5) Run generation on a single prompt -----\n",
    "# Use a prompt with enough context to steer the model:\n",
    "prompt_text = (\n",
    "    \"Explain why ChatGPT sometimes makes mistakes. \"\n",
    "    \"Consider the limitations of large language models and training data.\"\n",
    ")\n",
    "output_text = generate_autoregressive(\n",
    "    model_part1,\n",
    "    model_part2,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    max_new_tokens=100,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"PROMPT:\")\n",
    "print(prompt_text)\n",
    "print(\"\\nGENERATED TEXT:\")\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
